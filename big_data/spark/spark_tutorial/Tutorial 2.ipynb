{"cells":[{"cell_type":"code","source":["pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vUHtTFbz7yTE","executionInfo":{"status":"ok","timestamp":1724998764727,"user_tz":-120,"elapsed":36283,"user":{"displayName":"Akash Baidya","userId":"10886959240220178746"}},"outputId":"aeabe712-a48e-4ca4-ee45-6a91433ba891"},"id":"vUHtTFbz7yTE","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=d4c66496e1c36e241aed8f09bb1ad4e456bdb29b0248ebdb3f792e2d059b0b88\n","  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.2\n"]}]},{"cell_type":"markdown","id":"ded0783b","metadata":{"id":"ded0783b"},"source":["### Pyspark Handling Missing Values\n","- Dropping Columns\n","- Dropping Rows\n","- Various Parameter In Dropping functionalities\n","- Handling Missing values by Mean, MEdian And Mode"]},{"cell_type":"code","execution_count":3,"id":"805e7382","metadata":{"id":"805e7382","executionInfo":{"status":"ok","timestamp":1724998852776,"user_tz":-120,"elapsed":1296,"user":{"displayName":"Akash Baidya","userId":"10886959240220178746"}}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark=SparkSession.builder.appName('Practice').getOrCreate()"]},{"cell_type":"code","execution_count":4,"id":"e48ebc07","metadata":{"id":"e48ebc07","executionInfo":{"status":"ok","timestamp":1724998926693,"user_tz":-120,"elapsed":5722,"user":{"displayName":"Akash Baidya","userId":"10886959240220178746"}}},"outputs":[],"source":["df_pyspark=spark.read.csv('test2.csv',header=True,inferSchema=True)"]},{"cell_type":"code","execution_count":5,"id":"53ab7bbc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53ab7bbc","executionInfo":{"status":"ok","timestamp":1724998928668,"user_tz":-120,"elapsed":295,"user":{"displayName":"Akash Baidya","userId":"10886959240220178746"}},"outputId":"5356a38e-66f1-48e0-c6d4-9315dd03fcc8"},"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- Name: string (nullable = true)\n"," |-- age: integer (nullable = true)\n"," |-- Experience: integer (nullable = true)\n"," |-- Salary: integer (nullable = true)\n","\n"]}],"source":["df_pyspark.printSchema()"]},{"cell_type":"code","execution_count":6,"id":"ed677b30","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ed677b30","executionInfo":{"status":"ok","timestamp":1724998931332,"user_tz":-120,"elapsed":667,"user":{"displayName":"Akash Baidya","userId":"10886959240220178746"}},"outputId":"f06d2767-1d21-4ad1-a1b9-64263f23c652"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+----+----------+------+\n","|     Name| age|Experience|Salary|\n","+---------+----+----------+------+\n","|    Krish|  31|        10| 30000|\n","|Sudhanshu|  30|         8| 25000|\n","|    Sunny|  29|         4| 20000|\n","|     Paul|  24|         3| 20000|\n","|   Harsha|  21|         1| 15000|\n","|  Shubham|  23|         2| 18000|\n","|   Mahesh|NULL|      NULL| 40000|\n","|     NULL|  34|        10| 38000|\n","|     NULL|  36|      NULL|  NULL|\n","+---------+----+----------+------+\n","\n"]}],"source":["df_pyspark.show()"]},{"cell_type":"code","execution_count":7,"id":"523d3c4e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"523d3c4e","executionInfo":{"status":"ok","timestamp":1724998934512,"user_tz":-120,"elapsed":307,"user":{"displayName":"Akash Baidya","userId":"10886959240220178746"}},"outputId":"cc8d33f6-ef04-4141-e4dc-ffc6ea0493d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["+----+----------+------+\n","| age|Experience|Salary|\n","+----+----------+------+\n","|  31|        10| 30000|\n","|  30|         8| 25000|\n","|  29|         4| 20000|\n","|  24|         3| 20000|\n","|  21|         1| 15000|\n","|  23|         2| 18000|\n","|NULL|      NULL| 40000|\n","|  34|        10| 38000|\n","|  36|      NULL|  NULL|\n","+----+----------+------+\n","\n"]}],"source":["##drop the columns\n","df_pyspark.drop('Name').show()"]},{"cell_type":"code","execution_count":8,"id":"9c041e07","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9c041e07","executionInfo":{"status":"ok","timestamp":1724998936597,"user_tz":-120,"elapsed":305,"user":{"displayName":"Akash Baidya","userId":"10886959240220178746"}},"outputId":"6f15dbe7-640a-4fcb-a7ee-2c34fd492474"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+----+----------+------+\n","|     Name| age|Experience|Salary|\n","+---------+----+----------+------+\n","|    Krish|  31|        10| 30000|\n","|Sudhanshu|  30|         8| 25000|\n","|    Sunny|  29|         4| 20000|\n","|     Paul|  24|         3| 20000|\n","|   Harsha|  21|         1| 15000|\n","|  Shubham|  23|         2| 18000|\n","|   Mahesh|NULL|      NULL| 40000|\n","|     NULL|  34|        10| 38000|\n","|     NULL|  36|      NULL|  NULL|\n","+---------+----+----------+------+\n","\n"]}],"source":["df_pyspark.show()"]},{"cell_type":"code","execution_count":9,"id":"36845e38","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36845e38","executionInfo":{"status":"ok","timestamp":1724998939210,"user_tz":-120,"elapsed":334,"user":{"displayName":"Akash Baidya","userId":"10886959240220178746"}},"outputId":"2978f3eb-9ef5-4cb4-e764-906e2ea6fe36"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+---+----------+------+\n","|     Name|age|Experience|Salary|\n","+---------+---+----------+------+\n","|    Krish| 31|        10| 30000|\n","|Sudhanshu| 30|         8| 25000|\n","|    Sunny| 29|         4| 20000|\n","|     Paul| 24|         3| 20000|\n","|   Harsha| 21|         1| 15000|\n","|  Shubham| 23|         2| 18000|\n","+---------+---+----------+------+\n","\n"]}],"source":["df_pyspark.na.drop().show()"]},{"cell_type":"markdown","source":["Explanation:\n","df_pyspark.na.drop(how=\"any\"):\n","\n","- na: This is an alias to access functions for handling missing data.\n","drop(how=\"any\"): This method drops rows containing any null or missing values.\n","- how=\"any\": This means that if any value in a row is null, the entire row will be dropped.\n","If you used how=\"all\", it would only drop rows where all values are null.\n",".show():\n","\n","This method is used to display the DataFrame after the rows with missing values have been dropped."],"metadata":{"id":"e_r0b8rD9BcM"},"id":"e_r0b8rD9BcM"},{"cell_type":"code","execution_count":10,"id":"156e41cf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"156e41cf","executionInfo":{"status":"ok","timestamp":1724999044981,"user_tz":-120,"elapsed":380,"user":{"displayName":"Akash Baidya","userId":"10886959240220178746"}},"outputId":"3190ac6a-9d2b-4988-d95e-4db78536938d"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+---+----------+------+\n","|     Name|age|Experience|Salary|\n","+---------+---+----------+------+\n","|    Krish| 31|        10| 30000|\n","|Sudhanshu| 30|         8| 25000|\n","|    Sunny| 29|         4| 20000|\n","|     Paul| 24|         3| 20000|\n","|   Harsha| 21|         1| 15000|\n","|  Shubham| 23|         2| 18000|\n","+---------+---+----------+------+\n","\n"]}],"source":["### any==how\n","df_pyspark.na.drop(how=\"any\").show()"]},{"cell_type":"markdown","source":["Explanation:\n","- thresh=3: This means that a row must have at least 3 non-null values to be kept. If a row has fewer than 3 non-null values, it will be dropped.\n","- how=\"any\": This would typically drop any row with at least one null value. However, when thresh is set, the how parameter becomes less relevant because thresh takes precedence."],"metadata":{"id":"d3lz-27N9X9y"},"id":"d3lz-27N9X9y"},{"cell_type":"code","execution_count":null,"id":"e9af0da0","metadata":{"id":"e9af0da0","outputId":"8c04d7ea-788c-4fdc-e142-afcedebab97b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---+----------+------+\n","|     Name|age|Experience|Salary|\n","+---------+---+----------+------+\n","|    Krish| 31|        10| 30000|\n","|Sudhanshu| 30|         8| 25000|\n","|    Sunny| 29|         4| 20000|\n","|     Paul| 24|         3| 20000|\n","|   Harsha| 21|         1| 15000|\n","|  Shubham| 23|         2| 18000|\n","|     null| 34|        10| 38000|\n","|         |   |          |      |\n","+---------+---+----------+------+\n","\n"]}],"source":["##threshold\n","df_pyspark.na.drop(how=\"any\",thresh=3).show()"]},{"cell_type":"markdown","source":["Explanation:\n","- subset=['Age']: This means that only the Age column will be considered when deciding whether to drop a row. If a row has a null value in the Age column, it will be dropped.\n","\n","- how=\"any\": This indicates that if any of the columns specified in subset have a null value, the row will be dropped. However, since subset only contains one column (Age), this effectively means that any row where Age is null will be dropped."],"metadata":{"id":"vJ6KxZFz9vLk"},"id":"vJ6KxZFz9vLk"},{"cell_type":"code","execution_count":null,"id":"787fc949","metadata":{"id":"787fc949","outputId":"5851f849-e775-4bce-ad0f-2105d2e2bc4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+---+----------+------+\n","|     Name|age|Experience|Salary|\n","+---------+---+----------+------+\n","|    Krish| 31|        10| 30000|\n","|Sudhanshu| 30|         8| 25000|\n","|    Sunny| 29|         4| 20000|\n","|     Paul| 24|         3| 20000|\n","|   Harsha| 21|         1| 15000|\n","|  Shubham| 23|         2| 18000|\n","|     null| 34|        10| 38000|\n","|     null| 36|      null|  null|\n","|         |   |          |      |\n","+---------+---+----------+------+\n","\n"]}],"source":["##Subset\n","df_pyspark.na.drop(how=\"any\",subset=['Age']).show()"]},{"cell_type":"markdown","source":["Explanation:\n","- 'Missing Values': This is the value that will be used to replace any null values in the specified columns.\n","- ['Experience', 'age']: This is the list of columns where you want to fill the missing values with 'Missing Values'."],"metadata":{"id":"eQjYtbuG-IYt"},"id":"eQjYtbuG-IYt"},{"cell_type":"code","execution_count":null,"id":"72bad9ba","metadata":{"id":"72bad9ba","outputId":"6f62448f-b668-4505-ed1a-3dcd5ea31c58"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+--------------+--------------+------+\n","|     Name|           age|    Experience|Salary|\n","+---------+--------------+--------------+------+\n","|    Krish|            31|            10| 30000|\n","|Sudhanshu|            30|             8| 25000|\n","|    Sunny|            29|             4| 20000|\n","|     Paul|            24|             3| 20000|\n","|   Harsha|            21|             1| 15000|\n","|  Shubham|            23|             2| 18000|\n","|   Mahesh|Missing Values|Missing Values| 40000|\n","|     null|            34|            10| 38000|\n","|     null|            36|Missing Values|  null|\n","|         |              |              |      |\n","+---------+--------------+--------------+------+\n","\n"]}],"source":["### Filling the Missing Value\n","df_pyspark.na.fill('Missing Values',['Experience','age']).show()"]},{"cell_type":"code","execution_count":null,"id":"64e01bb9","metadata":{"id":"64e01bb9","outputId":"f1829769-615c-4d91-fc09-d377916d0f5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+----+----------+------+\n","|     Name| age|Experience|Salary|\n","+---------+----+----------+------+\n","|    Krish|  31|        10| 30000|\n","|Sudhanshu|  30|         8| 25000|\n","|    Sunny|  29|         4| 20000|\n","|     Paul|  24|         3| 20000|\n","|   Harsha|  21|         1| 15000|\n","|  Shubham|  23|         2| 18000|\n","|   Mahesh|null|      null| 40000|\n","|     null|  34|        10| 38000|\n","|     null|  36|      null|  null|\n","|         |    |          |      |\n","+---------+----+----------+------+\n","\n"]}],"source":["df_pyspark.show()"]},{"cell_type":"code","execution_count":null,"id":"b66832fd","metadata":{"id":"b66832fd","outputId":"413138de-a2cf-4dd0-ef4d-27840640f217"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- Name: string (nullable = true)\n"," |-- age: string (nullable = true)\n"," |-- Experience: string (nullable = true)\n"," |-- Salary: string (nullable = true)\n","\n"]}],"source":["df_pyspark.printSchema()"]},{"cell_type":"markdown","source":["Breakdown:\n","- from pyspark.ml.feature import Imputer:\n","\n","  This imports the Imputer class, which is used for handling missing values in PySpark DataFrames.\n","- inputCols=['age', 'Experience', 'Salary']:\n","\n","  Specifies the columns in the DataFrame that contain missing values that need to be imputed.\n","- outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]:\n","\n","  This dynamically generates a list of output column names where the imputed values will be stored. For example, the column age will have its imputed values stored in age_imputed.\n","- .setStrategy(\"median\"):\n","\n","  The strategy specifies how the missing values will be replaced. In this case, the missing values will be replaced with the median of the respective columns. Other strategies include \"mean\" and \"mode\"."],"metadata":{"id":"t4fKTpmx-UvO"},"id":"t4fKTpmx-UvO"},{"cell_type":"code","execution_count":11,"id":"e31190f2","metadata":{"id":"e31190f2","executionInfo":{"status":"ok","timestamp":1724999278256,"user_tz":-120,"elapsed":288,"user":{"displayName":"Akash Baidya","userId":"10886959240220178746"}}},"outputs":[],"source":["from pyspark.ml.feature import Imputer\n","\n","imputer = Imputer(\n","    inputCols=['age', 'Experience', 'Salary'],\n","    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n","    ).setStrategy(\"median\")"]},{"cell_type":"markdown","source":["Explanation:\n","- imputer.fit(df_pyspark): This computes the median (since setStrategy(\"median\") was used) for the columns Age, Experience, and Salary based on the data in df_pyspark.\n","\n","- transform(df_pyspark): This applies the computed median values to fill in any missing values in the original DataFrame, creating new columns with the suffix _imputed.\n","\n","- .show(): This displays the resulting DataFrame, including both the original columns and the new columns with imputed values."],"metadata":{"id":"KM9Fc2Rw-p2v"},"id":"KM9Fc2Rw-p2v"},{"cell_type":"code","execution_count":12,"id":"d84c4a3d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d84c4a3d","executionInfo":{"status":"ok","timestamp":1724999283136,"user_tz":-120,"elapsed":2136,"user":{"displayName":"Akash Baidya","userId":"10886959240220178746"}},"outputId":"55fb8adb-c74a-4f0b-c178-72b211cb96f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["+---------+----+----------+------+-----------+------------------+--------------+\n","|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n","+---------+----+----------+------+-----------+------------------+--------------+\n","|    Krish|  31|        10| 30000|         31|                10|         30000|\n","|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n","|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n","|     Paul|  24|         3| 20000|         24|                 3|         20000|\n","|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n","|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n","|   Mahesh|NULL|      NULL| 40000|         29|                 4|         40000|\n","|     NULL|  34|        10| 38000|         34|                10|         38000|\n","|     NULL|  36|      NULL|  NULL|         36|                 4|         20000|\n","+---------+----+----------+------+-----------+------------------+--------------+\n","\n"]}],"source":["# Add imputation cols to df\n","imputer.fit(df_pyspark).transform(df_pyspark).show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}